---
to: <%= e2eFileName %>.py
---
#!/usr/bin/env python3
"""
End-to-End <%= featureClass %> Ecosystem Demonstration

This script demonstrates the complete <%= featureClass %> ecosystem with:
- Weaver semantic conventions
- Multi-agent coordination
- OpenTelemetry integration
- Real-time monitoring
- Performance validation

Generated by: Weaver Hygen Templates (360¬∞ E2E Demo)
"""

import asyncio
import time
import json
from typing import Dict, Any, List
from pathlib import Path
from datetime import datetime

# Rich imports for beautiful output
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn
from rich.layout import Layout
from rich.live import Live

# Generated ecosystem imports
try:
    from src.dslmodel.ecosystems.<%= fileName %> import (
        <%= featureClass %>EcosystemManager,
        create_<%= featureLower %>_config,
        EcosystemConfig
    )
    ECOSYSTEM_AVAILABLE = True
except ImportError:
    ECOSYSTEM_AVAILABLE = False

# Weaver semantic conventions
try:
    from src.dslmodel.weaver.<%= domain %>_models import (
        create_<%= domain %>_span_attributes,
        create_swarm_<%= domain %>_context
    )
    WEAVER_MODELS_AVAILABLE = True
except ImportError:
    WEAVER_MODELS_AVAILABLE = False

console = Console()


class E2E<%= featureClass %>Demo:
    """End-to-end demonstration orchestrator for <%= featureClass %> ecosystem."""
    
    def __init__(self):
        self.console = Console()
        self.start_time = datetime.now()
        self.demo_results: Dict[str, Any] = {}
        self.span_file = Path("~/s2s/agent_coordination/telemetry_spans.jsonl").expanduser()
        
        # Demo configuration
        self.config = create_<%= featureLower %>_config(
            coordination_delay=0.3,  # Faster for demo
            processing_timeout=20.0,
            span_batch_size=50
        )
        
        # Demo phases
        self.phases = [
            "initialization",
            "semantic_conventions",
            "agent_coordination",
            "workflow_execution",
            "telemetry_validation",
            "performance_analysis"
        ]
    
    def check_prerequisites(self) -> bool:
        """Check if all prerequisites are available."""
        issues = []
        
        if not ECOSYSTEM_AVAILABLE:
            issues.append("‚ùå Ecosystem module not found - run hygen ecosystem-360 new first")
        
        if not WEAVER_MODELS_AVAILABLE:
            issues.append("‚ùå Weaver models not found - run hygen weaver-semconv new first")
        
        # Check span file directory
        if not self.span_file.parent.exists():
            try:
                self.span_file.parent.mkdir(parents=True, exist_ok=True)
                self.console.print(f"‚úÖ Created telemetry directory: {self.span_file.parent}")
            except Exception as e:
                issues.append(f"‚ùå Cannot create telemetry directory: {e}")
        
        if issues:
            self.console.print("[red]Prerequisites not met:[/red]")
            for issue in issues:
                self.console.print(f"  {issue}")
            return False
        
        return True
    
    async def run_complete_demo(self) -> Dict[str, Any]:
        """Execute the complete end-to-end demonstration."""
        if not self.check_prerequisites():
            return {"error": "Prerequisites not met"}
        
        # Create demo header
        self.console.print(Panel.fit(
            f"[bold cyan]üåü <%= featureClass %> Ecosystem - 360¬∞ E2E Demo[/bold cyan]\n"
            f"[dim]Domain: <%= domain %>[/dim]\n"
            f"[dim]Agents: <%= agents.join(', ') %>[/dim]\n"
            f"[dim]Workflows: <%= workflows.join(', ') %>[/dim]",
            border_style="cyan",
            title="End-to-End Demonstration"
        ))
        
        try:
            # Execute all demo phases
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                console=self.console
            ) as progress:
                
                main_task = progress.add_task("E2E Demo Progress", total=len(self.phases))
                
                for phase in self.phases:
                    progress.update(main_task, description=f"Executing {phase.replace('_', ' ').title()}...")
                    
                    phase_result = await self._execute_phase(phase)
                    self.demo_results[phase] = phase_result
                    
                    progress.update(main_task, advance=1)
                    
                    # Brief pause between phases
                    await asyncio.sleep(0.5)
            
            # Generate final report
            final_report = self._generate_final_report()
            self.demo_results["final_report"] = final_report
            
            # Display results
            self._display_demo_results()
            
            return self.demo_results
            
        except Exception as e:
            self.console.print(f"[red]‚ùå Demo failed: {e}[/red]")
            self.demo_results["error"] = str(e)
            return self.demo_results
    
    async def _execute_phase(self, phase: str) -> Dict[str, Any]:
        """Execute a specific demo phase."""
        phase_start = time.time()
        
        try:
            if phase == "initialization":
                return await self._phase_initialization()
            elif phase == "semantic_conventions":
                return await self._phase_semantic_conventions()
            elif phase == "agent_coordination":
                return await self._phase_agent_coordination()
            elif phase == "workflow_execution":
                return await self._phase_workflow_execution()
            elif phase == "telemetry_validation":
                return await self._phase_telemetry_validation()
            elif phase == "performance_analysis":
                return await self._phase_performance_analysis()
            else:
                return {"error": f"Unknown phase: {phase}"}
                
        except Exception as e:
            return {"error": str(e), "phase": phase}
        finally:
            duration = time.time() - phase_start
            self.console.print(f"[dim]  Phase {phase} completed in {duration:.2f}s[/dim]")
    
    async def _phase_initialization(self) -> Dict[str, Any]:
        """Phase 1: Initialize ecosystem and validate setup."""
        self.console.print("[blue]üìã Phase 1: Initialization[/blue]")
        
        # Create ecosystem manager
        self.ecosystem_manager = <%= featureClass %>EcosystemManager(self.config)
        
        # Get initial span count
        initial_spans = self._count_telemetry_spans()
        
        return {
            "ecosystem_created": True,
            "config": {
                "feature": self.config.feature_name,
                "domain": self.config.domain,
                "agents": self.config.agents,
                "workflows": self.config.workflows
            },
            "initial_spans": initial_spans,
            "span_file": str(self.span_file)
        }
    
    async def _phase_semantic_conventions(self) -> Dict[str, Any]:
        """Phase 2: Validate Weaver semantic conventions."""
        self.console.print("[blue]üèóÔ∏è  Phase 2: Semantic Conventions[/blue]")
        
        if not WEAVER_MODELS_AVAILABLE:
            return {"error": "Weaver models not available"}
        
        # Test semantic convention models
        try:
            # Create test span attributes
            span_attrs = create_<%= domain %>_span_attributes(
                operation_name="test_operation",
<% attributes.slice(0, 2).forEach(function(attr) { %>                <%= attr %>="test_<%= attr %>",
<% }); %>            )
            
            # Create complete context
            context = create_swarm_<%= domain %>_context(
                agent_name="test_agent",
                operation_name="test_operation",
                service_name="<%= featureLower %>_test_service"
            )
            
            # Convert to OTEL format
            otel_attrs = span_attrs.to_otel_attributes()
            complete_context = context.to_complete_otel_context()
            
            return {
                "span_attributes_valid": True,
                "context_valid": True,
                "otel_attributes": len(otel_attrs),
                "complete_context_keys": list(complete_context.keys())
            }
            
        except Exception as e:
            return {"error": f"Semantic convention validation failed: {e}"}
    
    async def _phase_agent_coordination(self) -> Dict[str, Any]:
        """Phase 3: Test agent coordination capabilities."""
        self.console.print("[blue]ü§ñ Phase 3: Agent Coordination[/blue]")
        
        # Get orchestrator
        orchestrator = self.ecosystem_manager.orchestrator
        
        # Test individual agent responses
        agent_tests = {}
        
        for agent_name in self.config.agents:
            agent = orchestrator.agents[agent_name]
            
            # Create test span
            test_span = self._create_test_span(agent_name)
            
            # Test agent response
            result = agent.forward(test_span)
            
            agent_tests[agent_name] = {
                "response": result is not None,
                "current_state": agent.state,
                "operations": getattr(agent, f"{agent_name}_operations", 0)
            }
        
        return {
            "agents_tested": len(agent_tests),
            "agent_results": agent_tests,
            "coordination_active": True
        }
    
    async def _phase_workflow_execution(self) -> Dict[str, Any]:
        """Phase 4: Execute complete workflows."""
        self.console.print("[blue]‚öôÔ∏è  Phase 4: Workflow Execution[/blue]")
        
        workflow_results = {}
        
        # Execute each workflow
        for workflow_name in self.config.workflows:
            try:
                workflow_start = time.time()
                
                result = await self.ecosystem_manager.orchestrator.execute_workflow(
                    workflow_name,
                    {"e2e_demo": True, "phase": "workflow_execution"}
                )
                
                workflow_results[workflow_name] = {
                    "success": True,
                    "duration": result.get("duration", 0),
                    "steps_completed": result.get("steps_completed", 0),
                    "agents_involved": result.get("agents_involved", [])
                }
                
            except Exception as e:
                workflow_results[workflow_name] = {
                    "success": False,
                    "error": str(e)
                }
        
        successful_workflows = sum(1 for r in workflow_results.values() if r.get("success"))
        
        return {
            "workflows_executed": len(workflow_results),
            "successful_workflows": successful_workflows,
            "success_rate": (successful_workflows / len(workflow_results)) * 100,
            "workflow_details": workflow_results
        }
    
    async def _phase_telemetry_validation(self) -> Dict[str, Any]:
        """Phase 5: Validate telemetry data."""
        self.console.print("[blue]üìä Phase 5: Telemetry Validation[/blue]")
        
        # Count spans after execution
        final_spans = self._count_telemetry_spans()
        initial_spans = self.demo_results.get("initialization", {}).get("initial_spans", 0)
        spans_generated = final_spans - initial_spans
        
        # Analyze span content
        span_analysis = self._analyze_telemetry_spans()
        
        return {
            "initial_spans": initial_spans,
            "final_spans": final_spans,
            "spans_generated": spans_generated,
            "span_file_size": self.span_file.stat().st_size if self.span_file.exists() else 0,
            "span_analysis": span_analysis
        }
    
    async def _phase_performance_analysis(self) -> Dict[str, Any]:
        """Phase 6: Analyze performance metrics."""
        self.console.print("[blue]üöÄ Phase 6: Performance Analysis[/blue]")
        
        # Get ecosystem report
        ecosystem_report = self.ecosystem_manager.orchestrator.generate_ecosystem_report()
        
        # Calculate demo duration
        demo_duration = (datetime.now() - self.start_time).total_seconds()
        
        # Performance metrics
        total_operations = sum(
            agent_metrics.get("operations", 0) 
            for agent_metrics in ecosystem_report.get("agents", {}).values()
        )
        
        avg_success_rate = sum(
            agent_metrics.get("success_rate", 0) 
            for agent_metrics in ecosystem_report.get("agents", {}).values()
        ) / max(len(ecosystem_report.get("agents", {})), 1)
        
        return {
            "demo_duration": demo_duration,
            "total_operations": total_operations,
            "avg_success_rate": avg_success_rate,
            "ecosystem_report": ecosystem_report,
            "performance_score": self._calculate_performance_score(ecosystem_report, demo_duration)
        }
    
    def _create_test_span(self, agent_name: str):
        """Create a test span for agent testing."""
        from src.dslmodel.agents.swarm.swarm_models import SpanData
        
        return SpanData(
            name=f"swarmsh.<%= domain %>.{agent_name}.test",
            trace_id=f"test_trace_{int(time.time() * 1000)}",
            span_id=f"test_span_{int(time.time() * 1000000)}",
            timestamp=time.time(),
            attributes={
                "test": True,
                "agent": agent_name,
                "e2e_demo": True
            }
        )
    
    def _count_telemetry_spans(self) -> int:
        """Count spans in telemetry file."""
        if not self.span_file.exists():
            return 0
        
        try:
            with self.span_file.open() as f:
                return sum(1 for line in f if line.strip())
        except Exception:
            return 0
    
    def _analyze_telemetry_spans(self) -> Dict[str, Any]:
        """Analyze telemetry span content."""
        if not self.span_file.exists():
            return {"error": "No telemetry file found"}
        
        span_types = {}
        agent_spans = {}
        
        try:
            with self.span_file.open() as f:
                for line in f:
                    if not line.strip():
                        continue
                    
                    try:
                        span = json.loads(line)
                        
                        # Count by span name prefix
                        name = span.get("name", "")
                        prefix = ".".join(name.split(".")[:3]) if "." in name else name
                        span_types[prefix] = span_types.get(prefix, 0) + 1
                        
                        # Count by agent
                        attrs = span.get("attributes", {})
                        agent = attrs.get("swarm.agent", attrs.get("agent", "unknown"))
                        agent_spans[agent] = agent_spans.get(agent, 0) + 1
                        
                    except json.JSONDecodeError:
                        continue
            
            return {
                "span_types": span_types,
                "agent_distribution": agent_spans,
                "total_unique_types": len(span_types),
                "total_agents": len(agent_spans)
            }
            
        except Exception as e:
            return {"error": f"Analysis failed: {e}"}
    
    def _calculate_performance_score(self, ecosystem_report: Dict[str, Any], duration: float) -> float:
        """Calculate overall performance score."""
        try:
            # Base score factors
            workflow_success = ecosystem_report.get("workflows", {}).get("success_rate", 0)
            avg_agent_success = sum(
                agent.get("success_rate", 0) 
                for agent in ecosystem_report.get("agents", {}).values()
            ) / max(len(ecosystem_report.get("agents", {})), 1)
            
            # Duration penalty (prefer faster demos)
            duration_score = max(0, 100 - (duration * 2))  # 2 points per second
            
            # Overall score
            score = (workflow_success * 0.4 + avg_agent_success * 0.4 + duration_score * 0.2)
            
            return min(100, max(0, score))
            
        except Exception:
            return 0.0
    
    def _generate_final_report(self) -> Dict[str, Any]:
        """Generate comprehensive final report."""
        performance = self.demo_results.get("performance_analysis", {})
        telemetry = self.demo_results.get("telemetry_validation", {})
        workflows = self.demo_results.get("workflow_execution", {})
        
        return {
            "demo_summary": {
                "feature": self.config.feature_name,
                "domain": self.config.domain,
                "duration": performance.get("demo_duration", 0),
                "phases_completed": len([p for p in self.demo_results if not self.demo_results[p].get("error")])
            },
            "success_metrics": {
                "workflow_success_rate": workflows.get("success_rate", 0),
                "performance_score": performance.get("performance_score", 0),
                "spans_generated": telemetry.get("spans_generated", 0),
                "agents_coordinated": len(self.config.agents)
            },
            "ecosystem_health": {
                "all_phases_passed": all(not result.get("error") for result in self.demo_results.values()),
                "telemetry_active": telemetry.get("final_spans", 0) > telemetry.get("initial_spans", 0),
                "coordination_working": workflows.get("successful_workflows", 0) > 0
            }
        }
    
    def _display_demo_results(self):
        """Display comprehensive demo results."""
        final_report = self.demo_results["final_report"]
        
        # Demo summary panel
        summary = final_report["demo_summary"]
        success_metrics = final_report["success_metrics"]
        
        summary_text = (
            f"[bold]Feature:[/bold] {summary['feature']}\n"
            f"[bold]Domain:[/bold] {summary['domain']}\n"
            f"[bold]Duration:[/bold] {summary['duration']:.2f}s\n"
            f"[bold]Phases:[/bold] {summary['phases_completed']}/{len(self.phases)}"
        )
        
        self.console.print(Panel(summary_text, title="Demo Summary", border_style="green"))
        
        # Success metrics table
        metrics_table = Table(title="Success Metrics")
        metrics_table.add_column("Metric", style="cyan")
        metrics_table.add_column("Value", style="green")
        
        metrics_table.add_row("Workflow Success Rate", f"{success_metrics['workflow_success_rate']:.1f}%")
        metrics_table.add_row("Performance Score", f"{success_metrics['performance_score']:.1f}/100")
        metrics_table.add_row("Spans Generated", str(success_metrics['spans_generated']))
        metrics_table.add_row("Agents Coordinated", str(success_metrics['agents_coordinated']))
        
        self.console.print(metrics_table)
        
        # Health check
        health = final_report["ecosystem_health"]
        health_status = "‚úÖ HEALTHY" if all(health.values()) else "‚ö†Ô∏è  ISSUES DETECTED"
        
        self.console.print(f"\n[bold]Ecosystem Health: {health_status}[/bold]")
        for check, status in health.items():
            icon = "‚úÖ" if status else "‚ùå"
            self.console.print(f"  {icon} {check.replace('_', ' ').title()}")


async def main():
    """Main entry point for E2E demo."""
    demo = E2E<%= featureClass %>Demo()
    
    console.print("[bold blue]üåü Starting <%= featureClass %> Ecosystem E2E Demo...[/bold blue]")
    
    try:
        results = await demo.run_complete_demo()
        
        if "error" in results:
            console.print(f"[red]‚ùå Demo failed: {results['error']}[/red]")
            return 1
        else:
            console.print("\n[green]üéâ E2E Demo completed successfully![/green]")
            
            # Save results
            results_file = Path(f"<%= featureLower %>_e2e_results.json")
            results_file.write_text(json.dumps(results, indent=2, default=str))
            console.print(f"[dim]Results saved to: {results_file}[/dim]")
            
            return 0
            
    except KeyboardInterrupt:
        console.print("\n[yellow]Demo interrupted by user[/yellow]")
        return 130
    except Exception as e:
        console.print(f"\n[red]Unexpected error: {e}[/red]")
        return 1


if __name__ == "__main__":
    exit(asyncio.run(main()))