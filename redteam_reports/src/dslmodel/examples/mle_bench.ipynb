{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-16T00:57:49.186120Z",
     "start_time": "2024-10-16T00:57:49.145887Z"
    }
   },
   "source": [
    "from datetime import datetime\n",
    "from faker import Faker\n",
    "from dslmodel import DSLModel\n",
    "from pydantic import Field\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "class MLE_Bench_CompetitionReport(DSLModel):\n",
    "    \"\"\"\n",
    "    A report summarizing the results of a submission in the MLE-bench competition. \n",
    "    This model tracks medals, scores, and leaderboard positions.\n",
    "    \n",
    "    Visit the official MLE-bench repository for details: \n",
    "    https://github.com/openai/mle-bench\n",
    "    \"\"\"\n",
    "\n",
    "    competition_id: str = Field(\n",
    "        \"{{ fake_uuid4() }}\", \n",
    "        description=\"Unique identifier for the competition. Each corresponds to an MLE-bench challenge.\"\n",
    "    )\n",
    "    score: float = Field(\n",
    "        \"{{ fake_pyfloat(left_digits=1, right_digits=5, positive=True) }}\", \n",
    "        description=\"Performance score of the submission, where lower or higher is better based on task (Section 2.3).\"\n",
    "    )\n",
    "    gold_threshold: float = Field(\n",
    "        \"{{ fake_pyfloat(left_digits=1, right_digits=5, positive=True) }}\", \n",
    "        description=\"Score required to earn a gold medal (Kaggle-like thresholds, see Table 1).\"\n",
    "    )\n",
    "    silver_threshold: float = Field(\n",
    "        \"{{ fake_pyfloat(left_digits=1, right_digits=5, positive=True) }}\", \n",
    "        description=\"Score required to earn a silver medal.\"\n",
    "    )\n",
    "    bronze_threshold: float = Field(\n",
    "        \"{{ fake_pyfloat(left_digits=1, right_digits=5, positive=True) }}\", \n",
    "        description=\"Score required to earn a bronze medal.\"\n",
    "    )\n",
    "    median_threshold: float = Field(\n",
    "        \"{{ fake_pyfloat(left_digits=1, right_digits=5, positive=True) }}\", \n",
    "        description=\"Median score of the leaderboard (benchmarking agent performance).\"\n",
    "    )\n",
    "    any_medal: bool = Field(\n",
    "        \"{{ fake_boolean() }}\", \n",
    "        description=\"True if any medal (gold, silver, or bronze) was awarded.\"\n",
    "    )\n",
    "    gold_medal: bool = Field(\n",
    "        \"{{ fake_boolean() }}\", \n",
    "        description=\"True if the submission achieved a gold medal.\"\n",
    "    )\n",
    "    silver_medal: bool = Field(\n",
    "        \"{{ fake_boolean() }}\", \n",
    "        description=\"True if the submission achieved a silver medal.\"\n",
    "    )\n",
    "    bronze_medal: bool = Field(\n",
    "        \"{{ fake_boolean() }}\", \n",
    "        description=\"True if the submission achieved a bronze medal.\"\n",
    "    )\n",
    "    above_median: bool = Field(\n",
    "        \"{{ fake_boolean() }}\", \n",
    "        description=\"True if the submission is above the leaderboard median.\"\n",
    "    )\n",
    "    submission_exists: bool = Field(\n",
    "        \"{{ fake_boolean() }}\", \n",
    "        description=\"True if the submission file exists.\"\n",
    "    )\n",
    "    valid_submission: bool = Field(\n",
    "        \"{{ fake_boolean() }}\", \n",
    "        description=\"True if the submission passed MLE-bench validation.\"\n",
    "    )\n",
    "    is_lower_better: bool = Field(\n",
    "        \"{{ fake_boolean() }}\", \n",
    "        description=\"True if lower scores indicate better performance (e.g., RMSE).\"\n",
    "    )\n",
    "    created_at: datetime = Field(\n",
    "        \"{{ fake_date_time_this_year() }}\", \n",
    "        description=\"Timestamp when the report was generated.\"\n",
    "    )\n",
    "    submission_path: str = Field(\n",
    "        \"{{ fake_file_path(extension='csv') }}\", \n",
    "        description=\"File path of the submission file.\"\n",
    "    )\n",
    "\n",
    "def generate_reports(num_reports=1):\n",
    "    \"\"\"Generate 8,000 synthetic competition reports using Jinja2 templates.\"\"\"\n",
    "    reports = [MLE_Bench_CompetitionReport() for _ in range(num_reports)]\n",
    "    print(f\"Generated {len(reports)} synthetic competition reports.\")\n",
    "    return reports"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T00:57:49.650064Z",
     "start_time": "2024-10-16T00:57:49.620243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "reports = generate_reports()\n",
    "print(reports[0].model_dump_json(indent=2))  # Display a sample report"
   ],
   "id": "f8c2c05609239021",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T00:57:50.567715Z",
     "start_time": "2024-10-16T00:57:50.559414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "from typing import Optional, List, Type\n",
    "\n",
    "from dspy.datasets.dataset import Dataset\n",
    "from dslmodel import DSLModel\n",
    "from dslmodel.readers.data_reader import DataReader\n",
    "from pydantic import ValidationError\n",
    "\n",
    "from dslmodel.spreadsheet import PandasSQLModel\n",
    "\n",
    "\n",
    "class DSLDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset class for MLE-bench competition reports with spreadsheet-like querying,\n",
    "    validation through DSLModels, and flexible data handling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dsl_model: Type[DSLModel],  # Required DSLModel to structure and validate data\n",
    "        file_path: Optional[Path] = None,  # Optional file path for loading external data\n",
    "        num_reports: int = 0,  # Number of synthetic reports to generate if no file is provided\n",
    "        train_size: float = 0.75,  # Train/Dev split ratio\n",
    "        seed: Optional[int] = None  # Seed for reproducibility\n",
    "    ):\n",
    "        \"\"\"Initialize the dataset with a DSLModel and either a file path or synthetic data.\"\"\"\n",
    "        super().__init__()\n",
    "        random.seed(seed)  # Ensure reproducibility\n",
    "\n",
    "        self.dsl_model = dsl_model  # Store the DSLModel class\n",
    "        self.reader = DataReader(file_path) if file_path else None\n",
    "\n",
    "        if file_path:\n",
    "            print(f\"📂 Loading data from: {file_path}\")\n",
    "            raw_data = self.reader.forward()\n",
    "            self.data = self._convert_to_model(raw_data)  # Convert to DSLModels\n",
    "        else:\n",
    "            print(f\"🧪 Generating {num_reports} synthetic reports...\")\n",
    "            self.data = [dsl_model() for _ in range(num_reports)]\n",
    "\n",
    "        self._train_ = []\n",
    "        self._dev_ = []\n",
    "        self.split(train_size)\n",
    "\n",
    "    def _convert_to_model(self, raw_data: List[dict]) -> List[DSLModel]:\n",
    "        \"\"\"Convert raw data to instances of the DSLModel.\"\"\"\n",
    "        typed_data = []\n",
    "        for row in raw_data:\n",
    "            try:\n",
    "                typed_data.append(self.dsl_model.model_validate(row))\n",
    "            except ValidationError as e:\n",
    "                print(f\"⚠️ Validation error: {e}\")\n",
    "                continue  # Skip invalid rows\n",
    "        return typed_data\n",
    "\n",
    "    def split(self, train_size: float = 0.75):\n",
    "        \"\"\"Split the dataset into train and dev sets.\"\"\"\n",
    "        split_index = int(len(self.data) * train_size)\n",
    "        self._train_ = self.data[:split_index]\n",
    "        self._dev_ = self.data[split_index:]\n",
    "        print(f\"📊 Dataset split: {len(self._train_)} train, {len(self._dev_)} dev\")\n",
    "\n",
    "    def resplit(self, train_size: float):\n",
    "        \"\"\"Allow re-splitting of the dataset at runtime.\"\"\"\n",
    "        self.split(train_size)\n",
    "\n",
    "    @property\n",
    "    def train(self) -> List[DSLModel]:\n",
    "        \"\"\"Return the train set.\"\"\"\n",
    "        return self._train_\n",
    "\n",
    "    @property\n",
    "    def dev(self) -> List[DSLModel]:\n",
    "        \"\"\"Return the dev set.\"\"\"\n",
    "        return self._dev_\n",
    "\n",
    "    def ask(self, prompt: str) -> List[DSLModel]:\n",
    "        \"\"\"Query the dataset using a natural language-like SQL prompt.\"\"\"\n",
    "        return self.from_prompt(prompt)\n",
    "\n",
    "    def from_prompt(self, prompt: str) -> List[DSLModel]:\n",
    "        \"\"\"Generate an SQL query from the prompt and retrieve matching results.\"\"\"\n",
    "        prompt = f\"{prompt}\\ncolumns: {self.dsl_model.field_names()}\"\n",
    "        query = PandasSQLModel.from_prompt(prompt).sqldf_query\n",
    "        raw_results = self.reader.forward(query=query) if self.reader else []\n",
    "        return self._convert_to_model(raw_results)\n",
    "\n",
    "    def get_random_sample(self) -> DSLModel:\n",
    "        \"\"\"Retrieve a random report from the dataset.\"\"\"\n",
    "        return random.choice(self.data)\n",
    "\n",
    "    def __getitem__(self, item) -> DSLModel:\n",
    "        \"\"\"Enable access by index.\"\"\"\n",
    "        return self.data[item]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the total number of reports.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Iterate over the dataset.\"\"\"\n",
    "        return iter(self.data)\n"
   ],
   "id": "79a168a3263fddbd",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T01:00:42.801609Z",
     "start_time": "2024-10-16T01:00:37.542665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dslmodel import init_instant, init_text\n",
    "\n",
    "\"\"\"Main function to demonstrate DSLDataset usage.\"\"\"\n",
    "# init_instant()\n",
    "init_text()\n",
    "# Initialize with synthetic reports using MLE_Bench_CompetitionReport as the DSL model\n",
    "dataset = DSLDataset(dsl_model=MLE_Bench_CompetitionReport, num_reports=100, seed=42)\n",
    "\n",
    "# Explore the dataset\n",
    "print(f\"Total reports: {len(dataset)}\")\n",
    "print(\"Random Sample:\")\n",
    "print(dataset.get_random_sample().model_dump_json(indent=2))\n",
    "\n",
    "# Query using prompts\n",
    "print(\"Query Result:\")\n",
    "print(dataset.ask(\"Select reports with gold_medal = true\"))\n",
    "\n",
    "# Train and dev set sizes\n",
    "print(f\"Train set size: {len(dataset.train)}\")\n",
    "print(f\"Dev set size: {len(dataset.dev)}\")\n"
   ],
   "id": "a765794b258d9776",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "68515c6aee2570fb",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
