#!/usr/bin/env python3
"""
Test Suite for Weaver-First Agent Worktree System
Validates complete OTEL-based coordination and evolution
"""

import asyncio
import tempfile
import shutil
import os
import time
import json
from pathlib import Path
from typing import List, Dict, Any
import subprocess
import pytest

# Test imports
from .generated.worktree_models import (
    WorktreeSpanFactory,
    AgentWorktreeCreateSpan,
    AgentTaskProgressSpan,
    FeatureIntegrationCompleteSpan
)
from .worktree_pipeline import AgentWorktreePipeline
from .evolution_system import AgentEvolutionSystem

# OTEL testing imports
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor, ConsoleSpanExporter
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter

class WeaverSystemTester:
    """
    Comprehensive test suite for weaver-first agent coordination
    Tests semantic conventions, generated models, pipeline, and evolution
    """
    
    def __init__(self):
        self.test_repo_path = None
        self.pipeline = None
        self.evolution_system = None
        self.test_results = {}
        
        # Initialize OTEL for testing
        self._setup_otel_testing()
    
    def _setup_otel_testing(self):
        """Setup OpenTelemetry for testing"""
        # Create tracer provider for testing
        trace.set_tracer_provider(TracerProvider())
        tracer_provider = trace.get_tracer_provider()
        
        # Add console exporter for test visibility
        console_processor = SimpleSpanProcessor(ConsoleSpanExporter())
        tracer_provider.add_span_processor(console_processor)
        
        print("âœ“ OTEL testing environment initialized")
    
    async def run_complete_test_suite(self) -> Dict[str, Any]:
        """Run all tests in the weaver-first system"""
        print("ðŸ§ª Starting Weaver-First Agent System Test Suite")
        print("=" * 60)
        
        try:
            # Setup test environment
            await self._setup_test_environment()
            
            # Test 1: Semantic Conventions Validation
            test1_result = await self._test_semantic_conventions()
            self.test_results["semantic_conventions"] = test1_result
            
            # Test 2: Generated Models Testing
            test2_result = await self._test_generated_models()
            self.test_results["generated_models"] = test2_result
            
            # Test 3: Worktree Pipeline Testing
            test3_result = await self._test_worktree_pipeline()
            self.test_results["worktree_pipeline"] = test3_result
            
            # Test 4: Agent Coordination Testing
            test4_result = await self._test_agent_coordination()
            self.test_results["agent_coordination"] = test4_result
            
            # Test 5: Evolution System Testing
            test5_result = await self._test_evolution_system()
            self.test_results["evolution_system"] = test5_result
            
            # Test 6: End-to-End Integration Testing
            test6_result = await self._test_end_to_end_integration()
            self.test_results["end_to_end"] = test6_result
            
            # Generate test report
            report = self._generate_test_report()
            
            return {
                "overall_success": all(result["success"] for result in self.test_results.values()),
                "test_results": self.test_results,
                "report": report
            }
            
        finally:
            await self._cleanup_test_environment()
    
    async def _setup_test_environment(self):
        """Setup isolated test environment"""
        print("ðŸ”§ Setting up test environment...")
        
        # Create temporary git repository
        self.test_repo_path = Path(tempfile.mkdtemp(prefix="weaver_test_"))
        os.chdir(self.test_repo_path)
        
        # Initialize git repo
        subprocess.run(["git", "init"], check=True, capture_output=True)
        subprocess.run(["git", "config", "user.email", "test@weaver.test"], check=True)
        subprocess.run(["git", "config", "user.name", "Weaver Test"], check=True)
        
        # Create initial commit
        (self.test_repo_path / "README.md").write_text("# Test Repository for Weaver System")
        subprocess.run(["git", "add", "."], check=True)
        subprocess.run(["git", "commit", "-m", "Initial commit"], check=True)
        
        # Initialize pipeline and evolution system
        self.pipeline = AgentWorktreePipeline(str(self.test_repo_path))
        self.evolution_system = AgentEvolutionSystem()
        
        print(f"âœ“ Test environment created at {self.test_repo_path}")
    
    async def _test_semantic_conventions(self) -> Dict[str, Any]:
        """Test 1: Validate semantic conventions are properly defined"""
        print("\nðŸ“‹ Test 1: Semantic Conventions Validation")
        print("-" * 40)
        
        start_time = time.time()
        
        try:
            # Check if semantic conventions file exists and is valid
            conventions_path = Path("semantic_conventions/worktree_pipeline.yaml")
            
            # Test weaver validation (simulated)
            validation_result = await self._validate_with_weaver(conventions_path)
            
            # Test span definitions completeness
            required_spans = [
                "agent.worktree.create",
                "agent.worktree.activate", 
                "agent.coordination.request",
                "agent.coordination.response",
                "agent.task.start",
                "agent.task.progress",
                "agent.task.complete",
                "feature.integration.start",
                "feature.integration.merge",
                "feature.integration.complete"
            ]
            
            span_coverage = self._check_span_coverage(required_spans)
            
            duration = time.time() - start_time
            
            success = validation_result and span_coverage >= 0.9
            
            print(f"  Weaver validation: {'âœ“' if validation_result else 'âœ—'}")
            print(f"  Span coverage: {span_coverage*100:.1f}%")
            print(f"  Duration: {duration:.2f}s")
            
            return {
                "success": success,
                "duration": duration,
                "weaver_validation": validation_result,
                "span_coverage": span_coverage,
                "details": f"Semantic conventions {'passed' if success else 'failed'} validation"
            }
            
        except Exception as e:
            return {
                "success": False,
                "duration": time.time() - start_time,
                "error": str(e),
                "details": "Semantic conventions test failed with exception"
            }
    
    async def _test_generated_models(self) -> Dict[str, Any]:
        """Test 2: Validate generated models work correctly"""
        print("\nðŸ—ï¸  Test 2: Generated Models Testing")
        print("-" * 40)
        
        start_time = time.time()
        
        try:
            # Test span creation
            span_factory = WorktreeSpanFactory()
            
            # Test each span type
            test_spans = []
            
            # Test worktree creation span
            create_span = span_factory.create_span(
                "agent.worktree.create",
                agent_id="test_agent",
                worktree_path="/tmp/test",
                feature_id="test_feature",
                branch_name="test_branch"
            )
            test_spans.append(("create", create_span))
            
            # Test progress span
            progress_span = span_factory.create_span(
                "agent.task.progress",
                agent_id="test_agent",
                task_id="test_task",
                progress_percentage=50.0,
                current_activity="testing"
            )
            test_spans.append(("progress", progress_span))
            
            # Test OTEL integration
            otel_integration_success = await self._test_otel_integration(test_spans)
            
            # Test span lifecycle
            lifecycle_success = await self._test_span_lifecycle(test_spans)
            
            duration = time.time() - start_time
            success = otel_integration_success and lifecycle_success
            
            print(f"  Span creation: {'âœ“' if test_spans else 'âœ—'}")
            print(f"  OTEL integration: {'âœ“' if otel_integration_success else 'âœ—'}")
            print(f"  Span lifecycle: {'âœ“' if lifecycle_success else 'âœ—'}")
            print(f"  Duration: {duration:.2f}s")
            
            return {
                "success": success,
                "duration": duration,
                "spans_tested": len(test_spans),
                "otel_integration": otel_integration_success,
                "lifecycle_test": lifecycle_success,
                "details": f"Generated models {'passed' if success else 'failed'} all tests"
            }
            
        except Exception as e:
            return {
                "success": False,
                "duration": time.time() - start_time,
                "error": str(e),
                "details": "Generated models test failed with exception"
            }
    
    async def _test_worktree_pipeline(self) -> Dict[str, Any]:
        """Test 3: Validate worktree pipeline functionality"""
        print("\nðŸ”„ Test 3: Worktree Pipeline Testing")
        print("-" * 40)
        
        start_time = time.time()
        
        try:
            # Test worktree creation
            agent_id = "test_pipeline_agent"
            feature_id = "test_pipeline_feature"
            branch_name = "test_pipeline_branch"
            
            worktree_path = await self.pipeline.create_agent_worktree(
                agent_id, feature_id, branch_name
            )
            worktree_created = Path(worktree_path).exists()
            
            # Test agent activation
            await self.pipeline.activate_agent(agent_id, ["python", "testing"])
            agent_status = self.pipeline.get_agent_status(agent_id)
            agent_activated = agent_status is not None
            
            # Test task assignment and progress
            task_id = "test_task_123"
            await self.pipeline.assign_task(agent_id, task_id, "Test task", 1000.0)
            await self.pipeline.report_progress(agent_id, task_id, 25.0, "Starting test")
            await self.pipeline.report_progress(agent_id, task_id, 75.0, "Almost done")
            
            # Test task completion
            await self.pipeline.complete_task(agent_id, task_id, "success", ["test_file.py"])
            
            # Test cleanup
            cleanup_success = await self.pipeline.cleanup_worktree(agent_id)
            
            duration = time.time() - start_time
            success = worktree_created and agent_activated and cleanup_success
            
            print(f"  Worktree creation: {'âœ“' if worktree_created else 'âœ—'}")
            print(f"  Agent activation: {'âœ“' if agent_activated else 'âœ—'}")
            print(f"  Task lifecycle: âœ“")
            print(f"  Cleanup: {'âœ“' if cleanup_success else 'âœ—'}")
            print(f"  Duration: {duration:.2f}s")
            
            return {
                "success": success,
                "duration": duration,
                "worktree_created": worktree_created,
                "agent_activated": agent_activated,
                "cleanup_success": cleanup_success,
                "details": f"Worktree pipeline {'passed' if success else 'failed'} functionality tests"
            }
            
        except Exception as e:
            return {
                "success": False,
                "duration": time.time() - start_time,
                "error": str(e),
                "details": "Worktree pipeline test failed with exception"
            }
    
    async def _test_agent_coordination(self) -> Dict[str, Any]:
        """Test 4: Validate agent coordination functionality"""
        print("\nðŸ¤ Test 4: Agent Coordination Testing")
        print("-" * 40)
        
        start_time = time.time()
        
        try:
            # Setup multiple agents
            agents = ["coord_agent_1", "coord_agent_2", "coord_agent_3"]
            feature_id = "coordination_test_feature"
            
            # Create worktrees for all agents
            for i, agent_id in enumerate(agents):
                branch_name = f"coord_branch_{i+1}"
                await self.pipeline.create_agent_worktree(agent_id, feature_id, branch_name)
                await self.pipeline.activate_agent(agent_id, ["coordination", "testing"])
            
            # Test coordination request
            coord_id = await self.pipeline.request_coordination(
                "coord_agent_1", 
                ["coord_agent_2", "coord_agent_3"],
                "Testing coordination mechanism"
            )
            
            coordination_created = coord_id is not None
            
            # Test coordination responses
            await self.pipeline.respond_to_coordination("coord_agent_2", coord_id, "accept")
            await self.pipeline.respond_to_coordination("coord_agent_3", coord_id, "accept")
            
            # Check coordination status
            coord_status = self.pipeline.get_coordination_status(coord_id)
            responses_received = len(coord_status["responses"]) == 2 if coord_status else False
            
            # Test feature integration
            integration_success = await self.pipeline.integrate_feature(
                feature_id, agents, "parallel"
            )
            
            # Cleanup all agents
            for agent_id in agents:
                await self.pipeline.cleanup_worktree(agent_id)
            
            duration = time.time() - start_time
            success = coordination_created and responses_received
            
            print(f"  Coordination request: {'âœ“' if coordination_created else 'âœ—'}")
            print(f"  Response handling: {'âœ“' if responses_received else 'âœ—'}")
            print(f"  Feature integration: {'âœ“' if integration_success else 'âœ—'}")
            print(f"  Duration: {duration:.2f}s")
            
            return {
                "success": success,
                "duration": duration,
                "coordination_created": coordination_created,
                "responses_received": responses_received,
                "integration_success": integration_success,
                "details": f"Agent coordination {'passed' if success else 'failed'} all tests"
            }
            
        except Exception as e:
            return {
                "success": False,
                "duration": time.time() - start_time,
                "error": str(e),
                "details": "Agent coordination test failed with exception"
            }
    
    async def _test_evolution_system(self) -> Dict[str, Any]:
        """Test 5: Validate evolution system functionality"""
        print("\nðŸ§¬ Test 5: Evolution System Testing")
        print("-" * 40)
        
        start_time = time.time()
        
        try:
            # Test evolution cycle
            cycle_id = self.evolution_system.start_evolution_cycle()
            evolution_completed = cycle_id is not None
            
            # Test pattern analysis (simulated data)
            patterns = self.evolution_system._analyze_coordination_patterns()
            patterns_discovered = len(patterns) > 0
            
            # Test optimization identification
            if patterns:
                optimizations = self.evolution_system._identify_optimizations(patterns)
                optimizations_found = len(optimizations) > 0
            else:
                optimizations_found = True  # No patterns, no optimizations needed
            
            # Test convention generation
            evolved_conventions = self.evolution_system._generate_evolved_conventions([])
            conventions_generated = "groups" in evolved_conventions
            
            duration = time.time() - start_time
            success = evolution_completed and patterns_discovered and conventions_generated
            
            print(f"  Evolution cycle: {'âœ“' if evolution_completed else 'âœ—'}")
            print(f"  Pattern analysis: {'âœ“' if patterns_discovered else 'âœ—'}")
            print(f"  Optimization ID: {'âœ“' if optimizations_found else 'âœ—'}")
            print(f"  Convention gen: {'âœ“' if conventions_generated else 'âœ—'}")
            print(f"  Duration: {duration:.2f}s")
            
            return {
                "success": success,
                "duration": duration,
                "cycle_id": cycle_id,
                "patterns_count": len(patterns) if patterns else 0,
                "conventions_generated": conventions_generated,
                "details": f"Evolution system {'passed' if success else 'failed'} functionality tests"
            }
            
        except Exception as e:
            return {
                "success": False,
                "duration": time.time() - start_time,
                "error": str(e),
                "details": "Evolution system test failed with exception"
            }
    
    async def _test_end_to_end_integration(self) -> Dict[str, Any]:
        """Test 6: End-to-end integration testing"""
        print("\nðŸŽ¯ Test 6: End-to-End Integration Testing")
        print("-" * 40)
        
        start_time = time.time()
        
        try:
            # Complete workflow simulation
            feature_id = "e2e_test_feature"
            agents = ["e2e_backend", "e2e_frontend", "e2e_database"]
            
            # Phase 1: Setup agents and worktrees
            setup_success = True
            for i, agent_id in enumerate(agents):
                try:
                    branch_name = f"e2e_branch_{agent_id}"
                    await self.pipeline.create_agent_worktree(agent_id, feature_id, branch_name)
                    await self.pipeline.activate_agent(agent_id, [f"capability_{i}"])
                except Exception:
                    setup_success = False
            
            # Phase 2: Assign and execute tasks
            task_execution_success = True
            if setup_success:
                for i, agent_id in enumerate(agents):
                    try:
                        task_id = f"e2e_task_{i}"
                        await self.pipeline.assign_task(agent_id, task_id, f"E2E task for {agent_id}")
                        
                        # Simulate progressive work
                        for progress in [25, 50, 75, 100]:
                            await self.pipeline.report_progress(
                                agent_id, task_id, progress, f"Progress at {progress}%"
                            )
                        
                        await self.pipeline.complete_task(agent_id, task_id, "success")
                    except Exception:
                        task_execution_success = False
            
            # Phase 3: Coordination between agents
            coordination_success = True
            if task_execution_success:
                try:
                    coord_id = await self.pipeline.request_coordination(
                        agents[0], agents[1:], "E2E coordination test"
                    )
                    
                    for agent_id in agents[1:]:
                        await self.pipeline.respond_to_coordination(agent_id, coord_id, "accept")
                    
                except Exception:
                    coordination_success = False
            
            # Phase 4: Feature integration
            integration_success = False
            if coordination_success:
                try:
                    integration_success = await self.pipeline.integrate_feature(
                        feature_id, agents, "sequential"
                    )
                except Exception:
                    integration_success = False
            
            # Phase 5: Evolution analysis
            evolution_success = True
            try:
                evolution_cycle = self.evolution_system.start_evolution_cycle()
            except Exception:
                evolution_success = False
            
            # Cleanup
            cleanup_success = True
            for agent_id in agents:
                try:
                    await self.pipeline.cleanup_worktree(agent_id)
                except Exception:
                    cleanup_success = False
            
            duration = time.time() - start_time
            overall_success = all([
                setup_success, task_execution_success, coordination_success,
                integration_success, evolution_success, cleanup_success
            ])
            
            print(f"  Agent setup: {'âœ“' if setup_success else 'âœ—'}")
            print(f"  Task execution: {'âœ“' if task_execution_success else 'âœ—'}")
            print(f"  Coordination: {'âœ“' if coordination_success else 'âœ—'}")
            print(f"  Integration: {'âœ“' if integration_success else 'âœ—'}")
            print(f"  Evolution: {'âœ“' if evolution_success else 'âœ—'}")
            print(f"  Cleanup: {'âœ“' if cleanup_success else 'âœ—'}")
            print(f"  Duration: {duration:.2f}s")
            
            return {
                "success": overall_success,
                "duration": duration,
                "phases": {
                    "setup": setup_success,
                    "task_execution": task_execution_success,
                    "coordination": coordination_success,
                    "integration": integration_success,
                    "evolution": evolution_success,
                    "cleanup": cleanup_success
                },
                "details": f"End-to-end integration {'passed' if overall_success else 'failed'}"
            }
            
        except Exception as e:
            return {
                "success": False,
                "duration": time.time() - start_time,
                "error": str(e),
                "details": "End-to-end integration test failed with exception"
            }
    
    # Utility testing methods
    async def _validate_with_weaver(self, conventions_path: Path) -> bool:
        """Simulate weaver validation"""
        # In real implementation, would run: weaver registry validate
        return True  # Simulated success
    
    def _check_span_coverage(self, required_spans: List[str]) -> float:
        """Check coverage of required spans in generated models"""
        try:
            available_spans = WorktreeSpanFactory.get_available_spans()
            covered = sum(1 for span in required_spans if span in available_spans)
            return covered / len(required_spans)
        except Exception:
            return 0.0
    
    async def _test_otel_integration(self, test_spans: List[tuple]) -> bool:
        """Test OpenTelemetry integration"""
        try:
            for span_name, span_obj in test_spans:
                # Test span creation
                otel_span = span_obj.start_span()
                if not otel_span:
                    return False
                
                # Test span ending
                span_obj.end_span(otel_span, success=True)
            
            return True
        except Exception:
            return False
    
    async def _test_span_lifecycle(self, test_spans: List[tuple]) -> bool:
        """Test complete span lifecycle"""
        try:
            for span_name, span_obj in test_spans:
                # Test attribute setting
                if hasattr(span_obj, 'agent_id'):
                    span_obj.agent_id = "test_agent"
                
                # Test from_attributes creation
                new_span = span_obj.__class__.from_attributes(agent_id="test_agent")
                if not new_span:
                    return False
            
            return True
        except Exception:
            return False
    
    def _generate_test_report(self) -> str:
        """Generate comprehensive test report"""
        total_tests = len(self.test_results)
        passed_tests = sum(1 for result in self.test_results.values() if result["success"])
        
        report = f"""
Weaver-First Agent System Test Report
=====================================

Overall Results:
- Tests Run: {total_tests}
- Tests Passed: {passed_tests}
- Tests Failed: {total_tests - passed_tests}
- Success Rate: {(passed_tests/total_tests)*100:.1f}%

Detailed Results:
"""
        
        for test_name, result in self.test_results.items():
            status = "âœ“ PASS" if result["success"] else "âœ— FAIL"
            duration = result.get("duration", 0)
            report += f"  {test_name}: {status} ({duration:.2f}s)\n"
            
            if not result["success"] and "error" in result:
                report += f"    Error: {result['error']}\n"
        
        # Performance summary
        total_duration = sum(result.get("duration", 0) for result in self.test_results.values())
        report += f"\nTotal Test Duration: {total_duration:.2f}s\n"
        
        # Recommendations
        if passed_tests < total_tests:
            report += "\nRecommendations:\n"
            for test_name, result in self.test_results.items():
                if not result["success"]:
                    report += f"- Fix {test_name}: {result.get('details', 'No details available')}\n"
        
        return report
    
    async def _cleanup_test_environment(self):
        """Cleanup test environment"""
        if self.test_repo_path and self.test_repo_path.exists():
            os.chdir("/")  # Change out of directory before deletion
            shutil.rmtree(self.test_repo_path)
            print(f"âœ“ Test environment cleaned up")

# Main test execution
async def run_weaver_system_tests():
    """Run complete weaver system tests"""
    tester = WeaverSystemTester()
    results = await tester.run_complete_test_suite()
    
    print("\n" + "=" * 60)
    print("ðŸ Test Suite Completed")
    print("=" * 60)
    print(results["report"])
    
    if results["overall_success"]:
        print("ðŸŽ‰ All tests passed! Weaver-first system is working correctly.")
    else:
        print("âŒ Some tests failed. Please review the results above.")
    
    return results

if __name__ == "__main__":
    # Run the complete test suite
    results = asyncio.run(run_weaver_system_tests())
    
    # Save results for analysis
    with open("weaver_test_results.json", "w") as f:
        json.dump(results, f, indent=2)
    
    print(f"\nðŸ“Š Test results saved to weaver_test_results.json")