#!/usr/bin/env python3
"""
Test Weaver + OpenTelemetry Integration

Comprehensive tests to verify that OpenTelemetry semantic conventions
generated by DSLModel match Weaver specifications and validation requirements.
"""

import asyncio
import json
import pytest
import subprocess
import tempfile
from pathlib import Path
from typing import Dict, List, Any

# Import our components
from dslmodel.commands.weaver_health_check import WeaverHealthChecker, HealthStatus
from dslmodel.utils.ollama_validator import OllamaValidator, safe_init_ollama


class TestWeaverOTELIntegration:
    """Test suite for Weaver + OTEL integration"""
    
    @pytest.fixture
    def health_checker(self):
        """Provide health checker instance"""
        return WeaverHealthChecker()
    
    @pytest.fixture
    def ollama_validator(self):
        """Provide Ollama validator instance"""
        return OllamaValidator()
    
    @pytest.fixture
    def sample_semconv(self):
        """Provide sample semantic convention YAML"""
        return """
groups:
  - id: trace.swarmsh.agent
    type: span
    brief: SwarmSH agent coordination telemetry
    note: >
      Defines telemetry for SwarmSH agent coordination system including
      agent lifecycle, work assignment, and completion tracking.
    spans:
      - id: swarmsh.agent.create
        stability: experimental
        brief: Agent creation in swarm coordination
        note: Emitted when a new agent is created in the swarm
        attributes:
          - id: agent.id
            type: string
            requirement_level: required
            brief: Unique identifier for the agent
            examples: ["agent_1671234567890123456"]
          - id: agent.name
            type: string
            requirement_level: required
            brief: Human-readable name for the agent
            examples: ["ProcessorAgent", "ValidatorAgent"]
          - id: agent.team
            type: string
            requirement_level: optional
            brief: Team assignment for the agent
            examples: ["alpha", "beta", "gamma"]
      
      - id: swarmsh.work.assign
        stability: experimental
        brief: Work assignment to agent
        note: Emitted when work is assigned to an agent
        attributes:
          - id: work.id
            type: string
            requirement_level: required
            brief: Unique identifier for the work item
            examples: ["work_a1b2c3d4"]
          - id: work.description
            type: string
            requirement_level: required
            brief: Description of the work to be performed
            examples: ["Process user validation", "Generate analytics report"]
          - id: work.priority
            type: string
            requirement_level: optional
            brief: Priority level of the work
            examples: ["high", "medium", "low"]
            enum: ["high", "medium", "low"]
          - id: agent.id
            type: string
            requirement_level: required
            brief: ID of the agent assigned to the work
            examples: ["agent_1671234567890123456"]
        
      - id: swarmsh.work.complete
        stability: experimental
        brief: Work completion by agent
        note: Emitted when an agent completes assigned work
        attributes:
          - id: work.id
            type: string
            requirement_level: required
            brief: Unique identifier for the completed work
            examples: ["work_a1b2c3d4"]
          - id: agent.id
            type: string
            requirement_level: required
            brief: ID of the agent that completed the work
            examples: ["agent_1671234567890123456"]
          - id: work.duration_ms
            type: int
            requirement_level: optional
            brief: Duration of work execution in milliseconds
            examples: [1500, 3200, 850]
          - id: work.status
            type: string
            requirement_level: required
            brief: Final status of the work
            examples: ["completed", "failed"]
            enum: ["completed", "failed"]
"""
    
    def test_ollama_availability(self, ollama_validator):
        """Test that Ollama is available for validation"""
        validation = ollama_validator.validate_configuration()
        
        assert validation["server_available"], "Ollama server must be available for tests"
        assert validation["models_accessible"], "Ollama models must be accessible"
        assert validation["default_model_available"], "Default model must be available"
    
    @pytest.mark.asyncio
    async def test_comprehensive_health_check(self, health_checker):
        """Test comprehensive health check execution"""
        system_health = await health_checker.run_comprehensive_health_check()
        
        # Verify health check completed
        assert system_health is not None
        assert len(system_health.checks) > 0
        assert system_health.overall_status in [s for s in HealthStatus]
        
        # Verify all expected checks ran
        check_names = [check.name for check in system_health.checks]
        expected_checks = [
            "Ollama Health",
            "Weaver Installation", 
            "Semantic Conventions",
            "OTEL Compatibility",
            "LLM-Powered Validation",
            "Generated Specifications",
            "DSLModel Integration"
        ]
        
        for expected in expected_checks:
            assert expected in check_names, f"Missing health check: {expected}"
    
    @pytest.mark.asyncio
    async def test_llm_semantic_validation(self, sample_semconv):
        """Test LLM-powered semantic convention validation"""
        # Initialize LLM
        success, lm, message = safe_init_ollama("ollama/qwen3")
        if not success:
            pytest.skip(f"Ollama not available: {message}")
        
        import dspy
        
        class SemanticValidator(dspy.Signature):
            """Validate OpenTelemetry semantic conventions for correctness."""
            yaml_content = dspy.InputField(desc="YAML semantic convention content")
            validation_result = dspy.OutputField(desc="Validation assessment with issues found")
        
        validator = dspy.ChainOfThought(SemanticValidator)
        result = validator(yaml_content=sample_semconv)
        
        # Verify LLM provided validation
        assert result.validation_result is not None
        assert len(result.validation_result) > 10  # Should provide meaningful feedback
        
        # Check for key validation elements
        validation_text = result.validation_result.lower()
        assert any(keyword in validation_text for keyword in [
            "valid", "correct", "compliant", "consistent", "well-formed"
        ]), "Validation should assess correctness"
    
    def test_weaver_yaml_structure(self, sample_semconv):
        """Test that generated YAML matches Weaver expected structure"""
        import yaml
        
        # Parse YAML
        try:
            data = yaml.safe_load(sample_semconv)
        except yaml.YAMLError as e:
            pytest.fail(f"Invalid YAML structure: {e}")
        
        # Verify top-level structure
        assert "groups" in data, "Missing 'groups' key in semantic conventions"
        assert isinstance(data["groups"], list), "Groups should be a list"
        
        # Verify group structure
        for group in data["groups"]:
            assert "id" in group, "Group missing 'id' field"
            assert "type" in group, "Group missing 'type' field"
            assert "brief" in group, "Group missing 'brief' field"
            
            if "spans" in group:
                # Verify span structure
                for span in group["spans"]:
                    assert "id" in span, "Span missing 'id' field"
                    assert "brief" in span, "Span missing 'brief' field"
                    assert "stability" in span, "Span missing 'stability' field"
                    
                    if "attributes" in span:
                        # Verify attribute structure
                        for attr in span["attributes"]:
                            assert "id" in attr, "Attribute missing 'id' field"
                            assert "type" in attr, "Attribute missing 'type' field"
                            assert "requirement_level" in attr, "Attribute missing 'requirement_level' field"
                            assert "brief" in attr, "Attribute missing 'brief' field"
    
    def test_weaver_validation_compatibility(self, sample_semconv):
        """Test that semantic conventions pass Weaver validation"""
        with tempfile.TemporaryDirectory() as temp_dir:
            # Write sample convention to file
            conv_file = Path(temp_dir) / "sample_convention.yaml"
            conv_file.write_text(sample_semconv)
            
            # Try to validate with weaver if available
            try:
                result = subprocess.run(
                    ["weaver", "registry", "check", "--registry", temp_dir],
                    capture_output=True,
                    text=True,
                    timeout=30
                )
                
                # If weaver is available, check validation results
                if result.returncode == 0:
                    # Passed validation
                    assert True
                else:
                    # Check if errors are acceptable (warnings vs errors)
                    stderr_lower = result.stderr.lower()
                    if "error" in stderr_lower and "fatal" in stderr_lower:
                        pytest.fail(f"Weaver validation failed: {result.stderr}")
                    # Warnings are acceptable for experimental conventions
                    
            except FileNotFoundError:
                pytest.skip("Weaver not available for validation")
            except subprocess.TimeoutExpired:
                pytest.skip("Weaver validation timed out")
    
    @pytest.mark.asyncio 
    async def test_generated_specs_quality(self):
        """Test quality of generated specifications"""
        # Look for generated specs
        potential_dirs = [
            Path("semconv_registry/generated"),
            Path("generated"),
            Path("output"),
            Path("demo_generated_feature")
        ]
        
        found_files = []
        for gen_dir in potential_dirs:
            if gen_dir.exists():
                yaml_files = list(gen_dir.glob("**/*.yaml"))
                yaml_files.extend(list(gen_dir.glob("**/*.yml")))
                found_files.extend(yaml_files)
        
        if not found_files:
            pytest.skip("No generated specifications found to test")
        
        # Test first few files for quality
        for yaml_file in found_files[:3]:  # Test up to 3 files
            try:
                content = yaml_file.read_text()
                
                # Basic structure checks
                assert len(content) > 50, f"File {yaml_file} appears too small"
                assert "groups" in content or "spans" in content, f"File {yaml_file} missing expected structure"
                
                # Try to parse as YAML
                import yaml
                data = yaml.safe_load(content)
                assert data is not None, f"File {yaml_file} contains invalid YAML"
                
            except Exception as e:
                pytest.fail(f"Generated file {yaml_file} failed quality check: {e}")
    
    def test_dslmodel_integration_modules(self):
        """Test that DSLModel integration modules are available"""
        integration_modules = [
            "dslmodel.utils.ollama_validator",
            "dslmodel.commands.weaver_health_check",
            "dslmodel.commands.ollama_validate"
        ]
        
        for module in integration_modules:
            try:
                __import__(module)
            except ImportError as e:
                pytest.fail(f"Integration module {module} not available: {e}")
    
    @pytest.mark.asyncio
    async def test_end_to_end_validation_flow(self, sample_semconv):
        """Test complete validation flow from YAML to Weaver to LLM"""
        with tempfile.TemporaryDirectory() as temp_dir:
            # 1. Write semantic convention
            conv_file = Path(temp_dir) / "test_convention.yaml"
            conv_file.write_text(sample_semconv)
            
            # 2. Test YAML parsing
            import yaml
            data = yaml.safe_load(sample_semconv)
            assert data is not None, "YAML parsing failed"
            
            # 3. Test Weaver validation (if available)
            weaver_result = None
            try:
                result = subprocess.run(
                    ["weaver", "registry", "check", "--registry", temp_dir],
                    capture_output=True,
                    text=True,
                    timeout=30
                )
                weaver_result = result.returncode == 0
            except (FileNotFoundError, subprocess.TimeoutExpired):
                weaver_result = None  # Weaver not available
            
            # 4. Test LLM validation
            llm_result = None
            try:
                success, lm, message = safe_init_ollama("ollama/qwen3")
                if success:
                    import dspy
                    
                    class Validator(dspy.Signature):
                        """Validate semantic conventions quality."""
                        content = dspy.InputField()
                        assessment = dspy.OutputField()
                    
                    validator = dspy.ChainOfThought(Validator)
                    result = validator(content=sample_semconv[:1000])  # Limit content
                    llm_result = len(result.assessment) > 10
                    
            except Exception:
                llm_result = None  # LLM not available
            
            # 5. Verify at least one validation method worked
            validation_methods = [
                ("YAML parsing", True),
                ("Weaver validation", weaver_result),
                ("LLM validation", llm_result)
            ]
            
            working_methods = [name for name, result in validation_methods if result is True]
            assert len(working_methods) >= 1, f"No validation methods worked. Available: {working_methods}"
    
    def test_health_check_json_output(self, health_checker):
        """Test that health check can produce valid JSON output"""
        async def run_test():
            system_health = await health_checker.run_comprehensive_health_check()
            
            # Convert to JSON-serializable format
            health_data = {
                "overall_status": system_health.overall_status.value,
                "summary": system_health.summary,
                "checks": [
                    {
                        "name": check.name,
                        "status": check.status.value,
                        "message": check.message,
                        "duration_ms": check.duration_ms,
                        "timestamp": check.timestamp.isoformat()
                    }
                    for check in system_health.checks
                ],
                "recommendations": system_health.recommendations,
                "timestamp": system_health.timestamp.isoformat()
            }
            
            # Verify JSON serialization works
            json_str = json.dumps(health_data, indent=2)
            assert len(json_str) > 100, "JSON output appears too small"
            
            # Verify JSON deserialization works
            parsed_data = json.loads(json_str)
            assert parsed_data["overall_status"] in ["healthy", "warning", "critical", "unknown"]
            assert "checks" in parsed_data
            assert len(parsed_data["checks"]) > 0
        
        asyncio.run(run_test())


# Utility functions for test setup
def setup_test_environment():
    """Setup test environment with sample data"""
    # Create test semconv registry
    test_registry = Path("test_semconv_registry")
    test_registry.mkdir(exist_ok=True)
    
    # Write sample convention
    sample_file = test_registry / "swarmsh_test.yaml"
    sample_file.write_text("""
groups:
  - id: trace.test
    type: span
    brief: Test semantic conventions
    spans:
      - id: test.operation
        brief: Test operation span
        stability: experimental
        attributes:
          - id: test.id
            type: string
            requirement_level: required
            brief: Test identifier
""")
    
    return test_registry


def cleanup_test_environment(test_registry: Path):
    """Clean up test environment"""
    import shutil
    if test_registry.exists():
        shutil.rmtree(test_registry)


if __name__ == "__main__":
    # Run tests directly
    pytest.main([__file__, "-v"])