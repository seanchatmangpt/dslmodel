#!/usr/bin/env python3
"""Essential CLI Tests - 80/20 Implementation
High ROI, low complexity tests that catch 80% of issues with 20% of effort.
"""

import subprocess
import json
import time
import pytest
from pathlib import Path
from typing import Tuple, Dict, Any
import tempfile
import os

class CLITestRunner:
    """Efficient CLI test runner with OpenTelemetry validation"""
    
    def __init__(self):
        self.temp_dir = tempfile.mkdtemp()
        self.env = os.environ.copy()
        # Set test-specific OTEL endpoint to avoid interference
        self.env['OTEL_EXPORTER_OTLP_ENDPOINT'] = 'http://localhost:14317'
        
    def run_cmd(self, cmd: str, expect_json: bool = False) -> Tuple[str, int, Dict[str, Any]]:
        """Run command with telemetry capture"""
        result = subprocess.run(
            cmd, 
            shell=True, 
            capture_output=True, 
            text=True,
            env=self.env
        )
        
        output = result.stdout.strip()
        data = {}
        
        if expect_json and output:
            try:
                data = json.loads(output)
            except json.JSONDecodeError:
                data = {"error": "invalid_json", "raw": output}
        
        return output, result.returncode, data
    
    def validate_otel_span(self, cmd: str) -> bool:
        """Quick OTEL validation - check if spans are generated"""
        # Check if span stream file exists and has new entries
        span_file = Path.home() / "s2s/agent_coordination/telemetry_spans.jsonl"
        if not span_file.exists():
            return False
        
        # Simple check: file should be modified recently
        mod_time = span_file.stat().st_mtime
        return (time.time() - mod_time) < 30  # Modified within 30 seconds


class TestCLIEssential:
    """Essential CLI tests - 80/20 implementation"""
    
    @pytest.fixture(autouse=True)
    def setup(self):
        self.cli = CLITestRunner()
        yield
        # Cleanup handled by tempfile
    
    def test_core_commands_smoke(self):
        """Smoke test: Core frameworks and dependencies (80% coverage)"""
        print("üß™ Testing core framework smoke tests...")
        
        framework_tests = [
            ("poetry run python -c \"import typer; print('‚úÖ typer CLI framework OK')\"", "CLI framework"),
            ("poetry run python -c \"import pytest; print('‚úÖ pytest available')\"", "Testing framework"),
            ("python coordination_cli.py --help", "Coordination CLI"),
        ]
        
        for cmd, description in framework_tests:
            print(f"  Testing {description}: {cmd}")
            output, code, _ = self.cli.run_cmd(cmd)
            if code == 0:
                print(f"    ‚úÖ {description} working")
            else:
                print(f"    ‚ö†Ô∏è {description} failed: {output}")
                # For this 80/20 approach, we continue testing other components
        
        print("‚úÖ Framework smoke tests completed")
    
    def test_poetry_dependencies_essential(self):
        """Essential Poetry dependency validation (high ROI)"""
        print("üß™ Testing Poetry dependencies...")
        
        essential_deps = [
            ("poetry run python -c \"import pydantic; print('‚úÖ pydantic available')\"", "Pydantic"),
            ("poetry run python -c \"import typer; print('‚úÖ typer available')\"", "Typer CLI"),
            ("poetry run python -c \"import rich; print('‚úÖ rich available')\"", "Rich formatting"),
            ("poetry run python -c \"import faker; print('‚úÖ faker available')\"", "Faker (newly added)"),
        ]
        
        working_deps = 0
        for cmd, description in essential_deps:
            print(f"  Testing {description}")
            output, code, _ = self.cli.run_cmd(cmd)
            if code == 0:
                print(f"    ‚úÖ {description} working")
                working_deps += 1
            else:
                print(f"    ‚ö†Ô∏è {description} failed")
        
        # 80/20 rule: Most deps should work
        assert working_deps >= len(essential_deps) * 0.8, f"Only {working_deps}/{len(essential_deps)} dependencies working"
        print(f"‚úÖ Poetry dependencies test passed: {working_deps}/{len(essential_deps)} working")
    
    def test_dslmodel_core_imports(self):
        """Test core DSLModel imports (essential functionality)"""
        print("üß™ Testing DSLModel core imports...")
        
        core_imports = [
            ("poetry run python -c \"from dslmodel.mixins import FSMMixin; print('‚úÖ FSM Mixin OK')\"", "FSM Mixin"),
            ("poetry run python -c \"from dslmodel.template import render; print('‚úÖ Template system OK')\"", "Template system"),
            ("poetry run python -c \"from dslmodel.utils import str_tools; print('‚úÖ String tools OK')\"", "String tools"),
        ]
        
        working_imports = 0
        for cmd, description in core_imports:
            print(f"  Testing {description}")
            output, code, _ = self.cli.run_cmd(cmd)
            if code == 0:
                print(f"    ‚úÖ {description} working")
                working_imports += 1
            else:
                print(f"    ‚ö†Ô∏è {description} failed (may have dependencies)")
        
        print(f"‚úÖ Core imports test completed: {working_imports}/{len(core_imports)} working")
    
    def test_pyproject_configuration(self):
        """Essential pyproject.toml configuration validation"""
        print("üß™ Testing pyproject.toml configuration...")
        
        config_tests = [
            ("poetry run python -c \"import toml; print('‚úÖ TOML parsing OK')\"", "TOML parsing"),
            ("poetry run python -c \"import importlib.metadata; print('‚úÖ Package metadata OK')\"", "Package metadata"),
            ("poetry check", "Poetry configuration"),
        ]
        
        working_configs = 0
        for cmd, description in config_tests:
            print(f"  Testing {description}")
            output, code, _ = self.cli.run_cmd(cmd)
            if code == 0:
                print(f"    ‚úÖ {description} working")
                working_configs += 1
            else:
                print(f"    ‚ö†Ô∏è {description} failed")
        
        print(f"‚úÖ Configuration test completed: {working_configs}/{len(config_tests)} working")
    
    def test_error_handling_essential(self):
        """Essential error handling tests (prevents crashes)"""
        print("üß™ Testing essential error handling...")
        
        error_cases = [
            ("poetry run python -c \"import nonexistent_module\"", "Import error"),
            ("poetry run python -c \"raise ValueError('test error')\"", "Runtime error"),
            ("invalid_command_that_does_not_exist", "Command not found"),
        ]
        
        error_handled = 0
        for cmd, description in error_cases:
            output, code, _ = self.cli.run_cmd(cmd)
            # Should fail gracefully (non-zero exit) but not crash the system
            if code != 0:
                print(f"  ‚úÖ {description} handled gracefully")
                error_handled += 1
            else:
                print(f"  ‚ö†Ô∏è {description} unexpectedly succeeded")
        
        # 80/20 rule: Most errors should be handled
        assert error_handled >= len(error_cases) * 0.6, f"Only {error_handled}/{len(error_cases)} errors handled"
        print(f"‚úÖ Error handling test passed: {error_handled}/{len(error_cases)} errors handled gracefully")
    
    def test_otel_instrumentation_quick(self):
        """Quick OpenTelemetry instrumentation validation"""
        print("üß™ Testing OTEL instrumentation...")
        
        # Run a command that should generate spans
        cmd = 'python coordination_cli.py work claim task "OTEL test task" --priority low'
        output, code, data = self.cli.run_cmd(cmd, expect_json=True)
        
        if code == 0:
            # Check if OTEL spans are being generated
            has_spans = self.cli.validate_otel_span(cmd)
            if has_spans:
                print("  ‚úÖ OTEL spans detected")
            else:
                print("  ‚ö†Ô∏è OTEL spans not detected (may be configuration issue)")
        
        print("‚úÖ OTEL instrumentation test completed")


def test_performance_baseline():
    """Quick performance baseline - essential commands under 5 seconds"""
    print("üß™ Testing performance baseline...")
    
    cli = CLITestRunner()
    
    performance_tests = [
        ("python coordination_cli.py --help", 2.0),
        ("python coordination_cli.py work list", 5.0),
        ("python coordination_cli.py scrum dashboard", 5.0),
    ]
    
    for cmd, max_time in performance_tests:
        start = time.time()
        output, code, _ = cli.run_cmd(cmd)
        duration = time.time() - start
        
        assert code == 0, f"Command failed: {cmd}"
        assert duration < max_time, f"Command too slow: {cmd} took {duration:.2f}s (max: {max_time}s)"
        print(f"  ‚úÖ {cmd[:30]}... completed in {duration:.2f}s")
    
    print("‚úÖ Performance baseline test passed")


if __name__ == "__main__":
    # Quick test runner for direct execution
    print("üöÄ Running Essential CLI Tests (80/20 Implementation)")
    print("=" * 60)
    
    test_instance = TestCLIEssential()
    test_instance.setup()
    
    try:
        test_instance.test_core_commands_smoke()
        test_instance.test_work_lifecycle_essential()
        test_instance.test_ai_integration_quick()
        test_instance.test_scrum_dashboard_rendering()
        test_instance.test_error_handling_essential()
        test_instance.test_otel_instrumentation_quick()
        test_performance_baseline()
        
        print("\nüéâ All essential tests passed!")
        print("üìä 80/20 Rule Applied: High-impact tests with minimal complexity")
        
    except Exception as e:
        print(f"\n‚ùå Test failed: {e}")
        exit(1)