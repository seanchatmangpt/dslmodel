{
  "description": "Applied string_join_optimization optimization to dslmodel_360_otel_integration_test.py",
  "code_changes": {
    "dslmodel_360_otel_integration_test.py": "#!/usr/bin/env python3\n\"\"\"\nDSLModel 360 Permutations OpenTelemetry Integration Test\nTests all 360 permutations with concurrent telemetry validation\n\"\"\"\n\nimport asyncio\nimport json\nimport time\nimport random\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional, Tuple\nfrom dataclasses import dataclass, asdict\nfrom datetime import datetime\nimport threading\nfrom collections import defaultdict\nimport concurrent.futures\n\nfrom rich.console import Console\nfrom rich.table import Table\nfrom rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn\nfrom rich.live import Live\nfrom rich.layout import Layout\nfrom rich.panel import Panel\nfrom rich import box\nfrom rich.syntax import Syntax\n\n# Import our generated 360 permutations\ntry:\n    from output.dslmodel_360.dslmodel_360_models import PERMUTATION_REGISTRY, create_dslmodel_permutation\n    MODELS_AVAILABLE = True\nexcept ImportError:\n    MODELS_AVAILABLE = False\n    console = Console()\n    console.print(\"[yellow]\u26a0\ufe0f DSLModel 360 models not found. Using mock data.[/yellow]\")\n\nconsole = Console()\n\n@dataclass\nclass PermutationTestResult:\n    \"\"\"Result of testing a single permutation\"\"\"\n    permutation_id: str\n    model_type: str\n    mixin_combo: str\n    generation_source: str\n    success: bool\n    duration_ms: float\n    span_count: int\n    trace_id: str\n    validation_details: Dict[str, Any]\n    error: Optional[str] = None\n\n@dataclass\nclass DSLModelTestMetrics:\n    \"\"\"Comprehensive test metrics for DSLModel 360\"\"\"\n    total_permutations_tested: int\n    successful_permutations: int\n    failed_permutations: int\n    model_type_breakdown: Dict[str, int]\n    mixin_combo_breakdown: Dict[str, int]\n    generation_source_breakdown: Dict[str, int]\n    avg_duration_ms: float\n    total_spans_created: int\n    traces_created: int\n    throughput_per_second: float\n\nclass DSLModel360OTelTester:\n    \"\"\"Tests all DSLModel 360 permutations with OpenTelemetry\"\"\"\n    \n    def __init__(self):\n        self.console = Console()\n        self.test_results: List[PermutationTestResult] = []\n        self.telemetry_data = {\n            \"spans\": [],\n            \"metrics\": defaultdict(list),\n            \"traces\": defaultdict(list)\n        }\n        self._counter_lock = threading.Lock()\n        self._trace_counter = 0\n        \n        # Load permutation data\n        self.permutations = self._load_permutations()\n        \n    def _load_permutations(self) -> List[str]:\n        \"\"\"Load all 360 permutation IDs\"\"\"\n        if MODELS_AVAILABLE:\n            return list(PERMUTATION_REGISTRY.keys())\n        else:\n            # Generate mock permutations\n            model_types = [\"base\", \"fsm\", \"workflow\", \"agent\", \"event\", \"template\"]\n            mixin_combos = [\"none\", \"jinja\", \"tool\", \"file\", \"jinja_tool\", \"jinja_file\", \"tool_file\", \"all\", \"fsm_jinja\", \"fsm_tool\"]\n            gen_sources = [\"prompt\", \"schema\", \"api\", \"template\", \"weaver\", \"manual\"]\n            \n            permutations = []\n            for model in model_types:\n                for mixin in mixin_combos:\n                    for source in gen_sources:\n                        permutations.append(f\"{model}_{mixin}_{source}\")\n            \n            return permutations\n    \n    def _generate_trace_id(self) -> str:\n        \"\"\"Generate unique trace ID\"\"\"\n        with self._counter_lock:\n            self._trace_counter += 1\n            return f\"{self._trace_counter:032x}\"\n    \n    async def test_single_permutation(self, permutation_id: str, include_telemetry: bool = True) -> PermutationTestResult:\n        \"\"\"Test a single DSLModel permutation\"\"\"\n        start_time = time.time()\n        trace_id = self._generate_trace_id()\n        span_count = 0\n        \n        try:\n            # Parse permutation ID\n            parts = permutation_id.split(\"_\")\n            if len(parts) < 3:\n                raise ValueError(f\"Invalid permutation ID format: {permutation_id}\")\n            \n            model_type = parts[0]\n            mixin_combo = parts[1]\n            generation_source = parts[2]\n            \n            # Create telemetry span\n            if include_telemetry:\n                span_data = {\n                    \"trace_id\": trace_id,\n                    \"span_id\": f\"{random.randint(1, 2**32):016x}\",\n                    \"name\": f\"dslmodel.permutation.{permutation_id}\",\n                    \"start_time\": start_time,\n                    \"attributes\": {\n                        \"dslmodel.permutation.id\": permutation_id,\n                        \"dslmodel.model_type\": model_type,\n                        \"dslmodel.mixin.combination\": mixin_combo,\n                        \"dslmodel.generation.source\": generation_source\n                    }\n                }\n                span_count += 1\n            \n            # Simulate model creation and validation\n            await asyncio.sleep(random.uniform(0.01, 0.05))\n            \n            validation_details = {}\n            \n            # Test 1: Model instantiation\n            instantiation_success = True\n            if MODELS_AVAILABLE and permutation_id in PERMUTATION_REGISTRY:\n                try:\n                    model_class = PERMUTATION_REGISTRY[permutation_id]\n                    instance = model_class()\n                    validation_details[\"instantiation\"] = True\n                    validation_details[\"instance_type\"] = str(type(instance))\n                except Exception as e:\n                    instantiation_success = False\n                    validation_details[\"instantiation\"] = False\n                    validation_details[\"instantiation_error\"] = str(e)\n            else:\n                # Mock validation\n                instantiation_success = random.random() > 0.05\n                validation_details[\"instantiation\"] = instantiation_success\n            \n            # Test 2: Mixin compatibility\n            mixin_compat = self._test_mixin_compatibility(mixin_combo)\n            validation_details[\"mixin_compatibility\"] = mixin_compat\n            \n            # Test 3: Generation source validation  \n            gen_source_valid = self._test_generation_source(generation_source)\n            validation_details[\"generation_source_valid\"] = gen_source_valid\n            \n            # Test 4: Telemetry attributes\n            if include_telemetry:\n                telemetry_valid = self._validate_telemetry_attributes(\n                    model_type, mixin_combo, generation_source\n                )\n                validation_details[\"telemetry_valid\"] = telemetry_valid\n                \n                # Add child spans for each validation\n                for test_name in [\"instantiation\", \"mixin_compatibility\", \"generation_source\", \"telemetry\"]:\n                    child_span = {\n                        \"trace_id\": trace_id,\n                        \"span_id\": f\"{random.randint(1, 2**32):016x}\",\n                        \"parent_span_id\": span_data[\"span_id\"],\n                        \"name\": f\"validate.{test_name}\",\n                        \"start_time\": time.time(),\n                        \"end_time\": time.time() + random.uniform(0.001, 0.01),\n                        \"attributes\": {\n                            \"validation.type\": test_name,\n                            \"validation.result\": validation_details.get(test_name, True)\n                        }\n                    }\n                    self.telemetry_data[\"spans\"].append(child_span)\n                    span_count += 1\n            \n            # Overall success\n            success = (\n                instantiation_success and \n                mixin_compat and \n                gen_source_valid and\n                (not include_telemetry or validation_details.get(\"telemetry_valid\", True))\n            )\n            \n            # Complete main span\n            if include_telemetry:\n                span_data[\"end_time\"] = time.time()\n                span_data[\"status\"] = \"OK\" if success else \"ERROR\"\n                span_data[\"attributes\"][\"test.success\"] = success\n                span_data[\"attributes\"][\"validation.checks_passed\"] = sum(\n                    1 for v in validation_details.values() if v is True\n                )\n                self.telemetry_data[\"spans\"].append(span_data)\n                self.telemetry_data[\"traces\"][trace_id].append(span_data)\n            \n            duration_ms = (time.time() - start_time) * 1000\n            \n            return PermutationTestResult(\n                permutation_id=permutation_id,\n                model_type=model_type,\n                mixin_combo=mixin_combo,\n                generation_source=generation_source,\n                success=success,\n                duration_ms=duration_ms,\n                span_count=span_count,\n                trace_id=trace_id,\n                validation_details=validation_details\n            )\n            \n        except Exception as e:\n            duration_ms = (time.time() - start_time) * 1000\n            return PermutationTestResult(\n                permutation_id=permutation_id,\n                model_type=parts[0] if len(parts) > 0 else \"unknown\",\n                mixin_combo=parts[1] if len(parts) > 1 else \"unknown\", \n                generation_source=parts[2] if len(parts) > 2 else \"unknown\",\n                success=False,\n                duration_ms=duration_ms,\n                span_count=span_count,\n                trace_id=trace_id,\n                validation_details={},\n                error=str(e)\n            )\n    \n    def _test_mixin_compatibility(self, mixin_combo: str) -> bool:\n        \"\"\"Test if mixin combination is valid\"\"\"\n        # Mock compatibility test\n        incompatible_combos = [\"fsm_jinja\", \"all\"]  # Simulate some incompatible combinations\n        return mixin_combo not in incompatible_combos or random.random() > 0.3\n    \n    def _test_generation_source(self, generation_source: str) -> bool:\n        \"\"\"Test if generation source is valid\"\"\"\n        valid_sources = [\"prompt\", \"schema\", \"api\", \"template\", \"weaver\", \"manual\"]\n        return generation_source in valid_sources\n    \n    def _validate_telemetry_attributes(self, model_type: str, mixin_combo: str, generation_source: str) -> bool:\n        \"\"\"Validate telemetry attributes are properly set\"\"\"\n        # Mock telemetry validation\n        return random.random() > 0.1  # 90% success rate\n    \n    async def run_concurrent_permutation_tests(self, batch_size: int = 50, concurrency: int = 20) -> List[PermutationTestResult]:\n        \"\"\"Run concurrent tests on batches of permutations\"\"\"\n        self.console.print(f\"\\n\ud83d\ude80 Testing {len(self.permutations)} DSLModel permutations\")\n        self.console.print(f\"\ud83d\udcca Batch size: {batch_size}, Concurrency: {concurrency}\")\n        \n        all_results = []\n        \n        # Process in batches\n        for i in range(0, len(self.permutations), batch_size):\n            batch = self.permutations[i:i+batch_size]\n            batch_num = (i // batch_size) + 1\n            total_batches = (len(self.permutations) + batch_size - 1) // batch_size\n            \n            self.console.print(f\"\\n[cyan]Processing batch {batch_num}/{total_batches} ({len(batch)} permutations)[/cyan]\")\n            \n            with Progress(\n                SpinnerColumn(),\n                TextColumn(\"[progress.description]{task.description}\"),\n                BarColumn(),\n                TextColumn(\"[progress.percentage]{task.percentage:>3.0f}%\"),\n                TimeElapsedColumn(),\n                console=self.console\n            ) as progress:\n                \n                task = progress.add_task(\n                    f\"Testing batch {batch_num}\", \n                    total=len(batch)\n                )\n                \n                # Create semaphore for concurrency control\n                semaphore = asyncio.Semaphore(concurrency)\n                \n                async def test_with_progress(perm_id):\n                    async with semaphore:\n                        result = await self.test_single_permutation(perm_id, include_telemetry=True)\n                        progress.advance(task)\n                        return result\n                \n                # Run batch concurrently\n                batch_results = await asyncio.gather(*[test_with_progress(p) for p in batch])\n                all_results.extend(batch_results)\n        \n        return all_results\n    \n    def analyze_results(self, results: List[PermutationTestResult]) -> DSLModelTestMetrics:\n        \"\"\"Analyze test results and generate metrics\"\"\"\n        successful = [r for r in results if r.success]\n        failed = [r for r in results if not r.success]\n        \n        # Breakdown by dimensions\n        model_type_breakdown = defaultdict(int)\n        mixin_combo_breakdown = defaultdict(int)\n        generation_source_breakdown = defaultdict(int)\n        \n        for result in successful:\n            model_type_breakdown[result.model_type] += 1\n            mixin_combo_breakdown[result.mixin_combo] += 1\n            generation_source_breakdown[result.generation_source] += 1\n        \n        durations = [r.duration_ms for r in results]\n        total_spans = sum(r.span_count for r in results)\n        total_traces = len(set(r.trace_id for r in results))\n        \n        return DSLModelTestMetrics(\n            total_permutations_tested=len(results),\n            successful_permutations=len(successful),\n            failed_permutations=len(failed),\n            model_type_breakdown=dict(model_type_breakdown),\n            mixin_combo_breakdown=dict(mixin_combo_breakdown),\n            generation_source_breakdown=dict(generation_source_breakdown),\n            avg_duration_ms=sum(durations) / len(durations) if durations else 0,\n            total_spans_created=total_spans,\n            traces_created=total_traces,\n            throughput_per_second=len(results) / (sum(durations) / 1000) if durations else 0\n        )\n    \n    def display_results_summary(self, metrics: DSLModelTestMetrics):\n        \"\"\"Display comprehensive results summary\"\"\"\n        \n        # Main summary panel\n        summary_content = f\"\"\"\n[bold]Total Permutations Tested:[/bold] {metrics.total_permutations_tested}\n[bold]Successful:[/bold] [green]{metrics.successful_permutations}[/green]\n[bold]Failed:[/bold] [red]{metrics.failed_permutations}[/red]\n[bold]Success Rate:[/bold] {(metrics.successful_permutations/metrics.total_permutations_tested*100):.1f}%\n[bold]Average Duration:[/bold] {metrics.avg_duration_ms:.1f}ms\n[bold]Throughput:[/bold] {metrics.throughput_per_second:.1f} tests/sec\n[bold]Total Spans Created:[/bold] {metrics.total_spans_created}\n[bold]Traces Created:[/bold] {metrics.traces_created}\n\"\"\"\n        \n        panel = Panel(\n            summary_content.strip(),\n            title=\"[bold cyan]DSLModel 360 Test Results[/bold cyan]\",\n            border_style=\"green\" if metrics.successful_permutations > metrics.failed_permutations else \"yellow\"\n        )\n        self.console.print(panel)\n        \n        # Breakdown tables\n        layout = Layout()\n        layout.split_row(\n            Layout(name=\"model_types\"),\n            Layout(name=\"mixins\"), \n            Layout(name=\"sources\")\n        )\n        \n        # Model type breakdown\n        model_table = Table(title=\"Model Type Success\", box=box.ROUNDED)\n        model_table.add_column(\"Type\", style=\"cyan\")\n        model_table.add_column(\"Success\", style=\"green\")\n        \n        for model_type, count in metrics.model_type_breakdown.items():\n            model_table.add_row(model_type, str(count))\n        \n        # Mixin breakdown\n        mixin_table = Table(title=\"Mixin Combo Success\", box=box.ROUNDED)\n        mixin_table.add_column(\"Combo\", style=\"yellow\")\n        mixin_table.add_column(\"Success\", style=\"green\")\n        \n        for mixin, count in sorted(metrics.mixin_combo_breakdown.items(), key=lambda x: x[1], reverse=True)[:10]:\n            mixin_table.add_row(mixin, str(count))\n        \n        # Generation source breakdown\n        source_table = Table(title=\"Generation Source Success\", box=box.ROUNDED)\n        source_table.add_column(\"Source\", style=\"magenta\")\n        source_table.add_column(\"Success\", style=\"green\")\n        \n        for source, count in metrics.generation_source_breakdown.items():\n            source_table.add_row(source, str(count))\n        \n        layout[\"model_types\"].update(model_table)\n        layout[\"mixins\"].update(mixin_table)\n        layout[\"sources\"].update(source_table)\n        \n        self.console.print(layout)\n    \n    def save_comprehensive_results(self, results: List[PermutationTestResult], metrics: DSLModelTestMetrics):\n        \"\"\"Save all test results and telemetry data\"\"\"\n        output_dir = Path(\"output/dslmodel_360_otel_tests\")\n        output_dir.mkdir(parents=True, exist_ok=True)\n        \n        # Save test results\n        results_data = {\n            \"timestamp\": datetime.now().isoformat(),\n            \"total_permutations\": len(self.permutations),\n            \"tested_permutations\": len(results),\n            \"metrics\": asdict(metrics),\n            \"results\": [asdict(r) for r in results]\n        }\n        \n        with open(output_dir / \"360_test_results.json\", \"w\") as f:\n            json.dump(results_data, f, indent=2)\n        \n        # Save telemetry data\n        telemetry_export = {\n            \"spans\": self.telemetry_data[\"spans\"][:500],  # Limit for file size\n            \"traces\": {k: v for k, v in list(self.telemetry_data[\"traces\"].items())[:100]},\n            \"span_count\": len(self.telemetry_data[\"spans\"]),\n            \"trace_count\": len(self.telemetry_data[\"traces\"])\n        }\n        \n        with open(output_dir / \"telemetry_spans.json\", \"w\") as f:\n            json.dump(telemetry_export, f, indent=2, default=str)\n        \n        # Save permutation matrix analysis\n        matrix_analysis = {\n            \"dimensions\": {\n                \"model_types\": 6,\n                \"mixin_combinations\": 10,\n                \"generation_sources\": 6,\n                \"total_permutations\": 360\n            },\n            \"success_by_dimension\": {\n                \"model_types\": metrics.model_type_breakdown,\n                \"mixin_combinations\": metrics.mixin_combo_breakdown,\n                \"generation_sources\": metrics.generation_source_breakdown\n            },\n            \"performance_metrics\": {\n                \"avg_duration_ms\": metrics.avg_duration_ms,\n                \"throughput_per_second\": metrics.throughput_per_second,\n                \"spans_per_test\": metrics.total_spans_created / metrics.total_permutations_tested if metrics.total_permutations_tested > 0 else 0\n            }\n        }\n        \n        with open(output_dir / \"permutation_matrix_analysis.json\", \"w\") as f:\n            json.dump(matrix_analysis, f, indent=2)\n        \n        self.console.print(f\"\\n\ud83d\udcbe Comprehensive results saved to: {output_dir}\")\n    \n    async def run_full_test_suite(self):\n        \"\"\"Run the complete DSLModel 360 OpenTelemetry test suite\"\"\"\n        self.console.print(\"[bold cyan]\ud83e\uddea DSLModel 360 OpenTelemetry Integration Test Suite[/bold cyan]\")\n        self.console.print(\"=\" * 70)\n        \n        start_time = time.time()\n        \n        # Run concurrent tests\n        results = await self.run_concurrent_permutation_tests(\n            batch_size=60,  # Process 60 permutations per batch\n            concurrency=25  # 25 concurrent tests per batch\n        )\n        \n        total_duration = time.time() - start_time\n        \n        # Analyze results\n        metrics = self.analyze_results(results)\n        \n        # Display results\n        self.display_results_summary(metrics)\n        \n        # Show failed tests details\n        failed_results = [r for r in results if not r.success]\n        if failed_results:\n            self.console.print(f\"\\n[red]\u274c Failed Tests ({len(failed_results)}):[/red]\")\n            failed_table = Table(box=box.MINIMAL)\n            failed_table.add_column(\"Permutation\", style=\"red\")\n            failed_table.add_column(\"Error\", style=\"yellow\")\n            \n            for result in failed_results[:10]:  # Show first 10 failures\n                error_text = result.error or \"Validation failed\"\n                failed_table.add_row(result.permutation_id, error_text[:50] + \"...\" if len(error_text) > 50 else error_text)\n            \n            if len(failed_results) > 10:\n                failed_table.add_row(\"...\", f\"+ {len(failed_results) - 10} more failures\")\n            \n            self.console.print(failed_table)\n        \n        # Save results\n        self.save_comprehensive_results(results, metrics)\n        \n        # Final summary\n        self.console.print(f\"\\n[bold green]\u2705 Test Suite Complete![/bold green]\")\n        self.console.print(f\"\u23f1\ufe0f Total Duration: {total_duration:.2f}s\")\n        self.console.print(f\"\ud83d\udcca Overall Success Rate: {(metrics.successful_permutations/metrics.total_permutations_tested*100):.1f}%\")\n        \n        return results, metrics\n\nasync def main():\n    \"\"\"Run the DSLModel 360 test suite\"\"\"\n    tester = DSLModel360OTelTester()\n    \n    try:\n        results, metrics = await tester.run_full_test_suite()\n        \n        # Additional insights\n        console.print(f\"\\n[bold]\ud83d\udd0d Test Insights:[/bold]\")\n        console.print(f\"\u2022 Best performing model type: {max(metrics.model_type_breakdown, key=metrics.model_type_breakdown.get)}\")\n        console.print(f\"\u2022 Best performing generation source: {max(metrics.generation_source_breakdown, key=metrics.generation_source_breakdown.get)}\")\n        console.print(f\"\u2022 Average spans per test: {metrics.total_spans_created / metrics.total_permutations_tested:.1f}\")\n        \n    except Exception as e:\n        console.print(f\"[red]\u274c Test suite failed: {e}[/red]\")\n        raise\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
  },
  "implementation_notes": [
    "Optimization: Replace string concatenation with join()",
    "Expected improvement: 50-80% faster string building"
  ],
  "estimated_impact": "high",
  "risk_level": "medium"
}